<app_name>/management/commands/
  seed_db.py
  seed.sql

the path and files above are standard paths if you want to setup to populate the app with default values.
to check for inclusion of command;

>> python manage.py
>> [store]
      seed_db
>> python manage.py seed_db


>> pipenv install pillow

>> pipenv install django-cors-headers

>> docker run --rm -it -p 3000:80 -p 2525:25 rnwood/smtp4dev

>> pipenv install django-templated-mail


celery
  - offloading task from main process, run it from background
  - send task to queue then several workers will pick from queue
  - can also schedule periodic tasks
  - message broker = responsible to reliably pass messages from app A to app B
  - cluster of message brokers = as failover
      redis (in-memory data store), also used as cache
      rabbitmq (enterprise grade, complex)

  run redis inside a docker container and in the background:
  port = localhost-port:docker-container-port
  6379 is standard port where redis is listening

  >> docker run -d -p 6379:6379 redis
  >> docker ps

  >> pipenv install redis

  >> pipenv install celery==4.4.7
  >> pipenv install eventlet==0.33.0
  >> pipenv install pypiwin32==223
  >> pipenv install gevent

  (actual command):
    >> celery worker -A storefront -E -l=info
  (command for dev, using hack):
    >> celery worker -A storefront --pool=gevent -E -l=debug

  note = whenever you create a new task for celery, restart it
         because autodiscovery doesn't work properly

  note2 = celery 4+ does not work on windows, need hack using environment variables:
    https://www.distributedpython.com/2018/08/21/celery-4-windows/
    Variable name: FORKED_BY_MULTIPROCESSING
    Variable value: 1

  note3 = in this example we are using celery 4, on latest celery (v5) the hack does not work
        = celery runs on linux.. need virtualization to setup on windows

  if worker is offline, the tasks will be queued by the message broker so that it gets executed later

  celery beat = task scheduler (manager/orchistrator)
  NOTE = you also need celery workers to handle the scheduled tasks

  (actual command):
    >> celery beat -A storefront -l=info
  (command for dev, using hack):
    >> celery beat -A storefront -l=debug


  flower = monitor celery tasks

  >> pipenv install flower
  >> celery -A storefront flower      #access it at localhost:5555


Automated Testing
  - write code to test endpoints and it's business rules
  - test behaviors (which is a constant) and NOT the implementation (because it can always change)
  - example of testing behavior:
      action: POST /collections
      if user==Anonymous return 401
      if user==Non-admin return 403
      if user==Admin && data==invalid return 400
      if user==Admin && data==valid return 200
  - testing frameworks:
      unittest
      pytest (ideal choice );
        more features, tons of plugins, less boilerplate
        note = pytest follows convention over configuration

      >> pipenv install --dev pytest         #only include it on development with --dev option, will not deploy
      >> pipenv install --dev pytest-django  #install plugin for django testing


  ~ setup pytest.ini to point to the django settings first

  execute all tests:
    >> pytest
  test specific directory:
    >> pytest store/tests
  test specific file:
    >> pytest store/tests/test_collections.py
  test specific class:
    >> pytest store/tests/test_collections.py::TestCreateCollection
  test pattern
    >> pytest -k "run-only-tests-with-this-term"


Continous Testing
  - run tests all the time
  - ideal if you have a powerful machine so tests complete really fast
  >> pipenv install --dev pytest-watch
  >> ptw


best practice:
  - test your code first before committing and deploying
  - test a single thing (single responsibility), but that thing might involve multiple assertions
  - always decouple your tests, do not use the existing value from database to test

note = you can configure Testing via VSCode, you can also debug tests there.. only problem is the lack of color coding

fixtures = remove duplication in testing code

to easily create models during testing;
>> pipenv install --dev model_bakery


Performance Testing (LOCUST)
  = create while building an application
  = to uncover all hidden performance problems
  = identify and fix potential performance problems
  = simulate users browsing the website

for performance testing;
>> pipenv install --dev locust

core use cases for performance testing in this project:
- browse products
- register, sign in, sign out

>> locust -f locustfiles/browse_products.py
note = access it at http://localhost:8089/


Performance Optimization Techniques
  - optimize python code
  examples:
    1. preload related objects
        Product.objects.select_related('...')
        Product.objects.prefetch_related('...')
    2. load only what you need
        Product.objects.only('title')
        Product.objects.defer('description')  // opposite of .only()
    3. use values (cheaper than creating dictionary or list compared to object)
        Product.objects.values()              // get dictionary
        Product.objects.values_list()         // get list
    4. count properly
        Product.objects.count()
    5. bulk create/update
        Product.objects.bulk_create([])

  - re-write the query (using SQL)
  - tune the database (redesign tables and relationships)
  - cache the result (caching, store result in memory and all subsequent request will read from memory)
  - buy more hardware


Profiling

  django-silk (get execution profile)
    >> pipenv install --dev django-silk
    ~ update settings
    >> python manage.py migrate
    ~ localhost:8000/silk/


Stress Testing
  - find upper limits of web application
  - only run on production environment

  json.decoder.JSONDecodeError while using locust at beyond 90 total users?
  found error: ConnectionRefusedError(10061, '[WinError 10061] No connection could be made because the target machine actively refused it.')
  you will need to restart django app to continue testing

  it seems you need to change the local machine's maximum mysql connections?
  or maybe because of mysql type of installation? developer default might mean limited connection?


Caching
  - if first time executed, get result, store to memory
  - not ideal if you cache data that is frequently being changed in db
  - problem: stale, out of date data
  - needs alot of memory
  - if db executes too fast, don't cache.. so you need to do some profiling to see what to cache

  Cache Backends
  - local memory (default)
  - memcached
  - redis
  - database
  - file system

performance test -> collect baseline

-d = detached mode / run in background
>> docker run -d -p 6379:6379 redis
>> pipenv install django-redis


manage redis;

>> docker ps
>> docker exec -it fd65df91e78b redis-cli
>> select 2
>> keys *
>> del (:... key name)      // delete specific key
>> flushall                 // empty all cache



collect static files for production
  settings.py;
    STATIC_URL = '/static/'
    STATIC_ROOT = os.path.join(BASE_DIR, 'static')
  command;
    >> python manage.py collectstatic

  django does not support serving static files,
  need to install this:

    >> pipenv install whitenoise


logging is important to see if there are problems during production
severity of log message (in order of importance per level):
  DEBUG
  INFO
  WARNING
  ERROR
  CRITICAL